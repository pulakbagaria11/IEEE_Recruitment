{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UwZmhbwdXgy"
      },
      "outputs": [],
      "source": [
        "import os, numpy as np, matplotlib.pyplot as plt\n",
        "folder = \"./fashion-minst/final/\" # folder containing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QgePQ3CdXg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5c6cfb9-6e5e-4c20-b516-683c2cbd3eaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 8 class\n",
            "Loaded 1 class\n",
            "Loaded 3 class\n",
            "Loaded 2 class\n",
            "Loaded 7 class\n",
            "Loaded 9 class\n",
            "Loaded 0 class\n",
            "Loaded 4 class\n",
            "Loaded 6 class\n",
            "Loaded 5 class\n",
            "After reshaping\n",
            "(60000, 784) (60000, 10)\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00784314\n",
            " 0.         0.         0.         0.34509805 0.42352942 0.4392157\n",
            " 0.43529412 0.24313726 0.         0.         0.00392157 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.02352941 0.         0.\n",
            " 0.7019608  0.5568628  0.         0.         0.02745098 0.39215687\n",
            " 0.4745098  0.         0.         0.00392157 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.00392157 0.         0.37254903 0.49019608 0.\n",
            " 0.         0.         0.         0.         0.4        0.31764707\n",
            " 0.         0.00784314 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.00392157 0.00392157 0.\n",
            " 0.         0.57254905 0.         0.         0.         0.\n",
            " 0.00392157 0.         0.         0.4745098  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.00784314 0.00784314\n",
            " 0.00784314 0.00392157 0.00784314 0.         0.25490198 0.5529412\n",
            " 0.         0.01176471 0.         0.         0.01176471 0.00392157\n",
            " 0.         0.35686275 0.13725491 0.         0.00784314 0.\n",
            " 0.00784314 0.00392157 0.00392157 0.         0.00392157 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.34509805 0.2        0.         0.\n",
            " 0.         0.         0.         0.         0.         0.21960784\n",
            " 0.21960784 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00784314\n",
            " 0.09411765 0.07450981 0.09019608 0.09019608 0.08627451 0.04313726\n",
            " 0.45490196 0.20392157 0.01176471 0.06666667 0.08627451 0.07843138\n",
            " 0.07843138 0.07843138 0.         0.25490198 0.3254902  0.\n",
            " 0.05882353 0.04313726 0.02745098 0.02352941 0.00392157 0.\n",
            " 0.         0.00784314 0.         0.1882353  0.45882353 0.4392157\n",
            " 0.40784314 0.39215687 0.45882353 0.46666667 0.5137255  0.53333336\n",
            " 0.46666667 0.36078432 0.39215687 0.36078432 0.4        0.4\n",
            " 0.5019608  0.50980395 0.6392157  0.4745098  0.4745098  0.45490196\n",
            " 0.46666667 0.5764706  0.45882353 0.         0.         0.00392157\n",
            " 0.         0.17254902 0.47058824 0.3372549  0.27450982 0.24313726\n",
            " 0.37254903 0.37254903 0.1882353  0.30588236 0.40784314 0.34509805\n",
            " 0.44313726 0.4117647  0.44313726 0.27450982 0.45490196 0.23529412\n",
            " 0.2509804  0.41960785 0.34509805 0.28627452 0.3019608  0.4509804\n",
            " 0.41960785 0.         0.00392157 0.         0.         0.1882353\n",
            " 0.52156866 0.29411766 0.29411766 0.25490198 0.39215687 0.34901962\n",
            " 0.11764706 0.34509805 0.50980395 0.1882353  0.3019608  0.31764707\n",
            " 0.33333334 0.22745098 0.47058824 0.21960784 0.17254902 0.40784314\n",
            " 0.35686275 0.3254902  0.31764707 0.43529412 0.45882353 0.14117648\n",
            " 0.00392157 0.         0.         0.21176471 0.50980395 0.30980393\n",
            " 0.3019608  0.26666668 0.37254903 0.7607843  0.74509805 0.8392157\n",
            " 0.4745098  0.21960784 0.28627452 0.26666668 0.26666668 0.21960784\n",
            " 0.57254905 0.7921569  0.77254903 0.8117647  0.31764707 0.3372549\n",
            " 0.3372549  0.39215687 0.5686275  0.5568628  0.00392157 0.\n",
            " 0.         0.6117647  0.40784314 0.33333334 0.30980393 0.30980393\n",
            " 0.2784314  0.31764707 0.28627452 0.31764707 0.23529412 0.31764707\n",
            " 0.30980393 0.31764707 0.31764707 0.28627452 0.27450982 0.3882353\n",
            " 0.4        0.36862746 0.31764707 0.37254903 0.34117648 0.44313726\n",
            " 0.4862745  0.05098039 0.         0.         0.46666667 0.78431374\n",
            " 0.30980393 0.34509805 0.30980393 0.31764707 0.29411766 0.2784314\n",
            " 0.29411766 0.27450982 0.33333334 0.30588236 0.30980393 0.31764707\n",
            " 0.30980393 0.31764707 0.28627452 0.29411766 0.29411766 0.30588236\n",
            " 0.37254903 0.3764706  0.3764706  0.4392157  0.56078434 0.09411765\n",
            " 0.         0.         0.85490197 0.73333335 0.31764707 0.3372549\n",
            " 0.3019608  0.31764707 0.3372549  0.3254902  0.33333334 0.31764707\n",
            " 0.31764707 0.31764707 0.31764707 0.31764707 0.3254902  0.31764707\n",
            " 0.34509805 0.36862746 0.36078432 0.38039216 0.36862746 0.3764706\n",
            " 0.40392157 0.43529412 0.5764706  0.13725491 0.         0.3254902\n",
            " 0.85882354 0.4509804  0.43529412 0.3254902  0.30980393 0.34117648\n",
            " 0.33333334 0.31764707 0.3254902  0.3372549  0.34509805 0.34901962\n",
            " 0.34509805 0.34901962 0.34901962 0.35686275 0.38039216 0.3764706\n",
            " 0.38039216 0.3764706  0.4        0.4117647  0.4        0.46666667\n",
            " 0.5568628  0.25490198 0.         0.8862745  0.5254902  0.13725491\n",
            " 0.5568628  0.30588236 0.3254902  0.34117648 0.3372549  0.34509805\n",
            " 0.35686275 0.35686275 0.36078432 0.36862746 0.37254903 0.37254903\n",
            " 0.3764706  0.3764706  0.38039216 0.38039216 0.3882353  0.3882353\n",
            " 0.4        0.41960785 0.40392157 0.46666667 0.5411765  0.36862746\n",
            " 0.         1.         0.07450981 0.3764706  0.53333336 0.3254902\n",
            " 0.34901962 0.35686275 0.36078432 0.36862746 0.36862746 0.37254903\n",
            " 0.37254903 0.37254903 0.38039216 0.38039216 0.3882353  0.3882353\n",
            " 0.39215687 0.39215687 0.4        0.4        0.40392157 0.41960785\n",
            " 0.42352942 0.45490196 0.53333336 0.4392157  0.21176471 0.85490197\n",
            " 0.         0.50980395 0.5137255  0.34901962 0.3882353  0.37254903\n",
            " 0.3882353  0.3882353  0.3882353  0.3882353  0.39215687 0.3882353\n",
            " 0.3882353  0.39215687 0.39215687 0.4        0.4        0.40392157\n",
            " 0.40784314 0.40784314 0.40784314 0.42352942 0.4392157  0.4509804\n",
            " 0.5411765  0.4        0.30588236 0.63529414 0.         0.56078434\n",
            " 0.53333336 0.34901962 0.4        0.4        0.4117647  0.4117647\n",
            " 0.40784314 0.4117647  0.4117647  0.4        0.4        0.4\n",
            " 0.40784314 0.4        0.40784314 0.40784314 0.40784314 0.40392157\n",
            " 0.4        0.42745098 0.43529412 0.45882353 0.5568628  0.3372549\n",
            " 0.28627452 0.5411765  0.         0.5058824  0.5686275  0.34509805\n",
            " 0.40784314 0.42745098 0.42352942 0.4392157  0.42352942 0.43529412\n",
            " 0.4117647  0.40392157 0.40392157 0.40392157 0.40784314 0.39215687\n",
            " 0.40784314 0.4117647  0.41960785 0.41960785 0.4117647  0.4392157\n",
            " 0.43529412 0.45882353 0.5529412  0.35686275 0.05098039 0.4862745\n",
            " 0.         0.4392157  0.6117647  0.3372549  0.41960785 0.4392157\n",
            " 0.4117647  0.39215687 0.40392157 0.41960785 0.40392157 0.39215687\n",
            " 0.39215687 0.39215687 0.41960785 0.41960785 0.4        0.40392157\n",
            " 0.41960785 0.42352942 0.42352942 0.4509804  0.4392157  0.4392157\n",
            " 0.53333336 0.41960785 0.         0.34901962 0.7647059  0.5568628\n",
            " 0.67058825 0.5058824  0.6039216  0.63529414 0.65882355 0.62352943\n",
            " 0.62352943 0.60784316 0.5686275  0.46666667 0.38039216 0.37254903\n",
            " 0.37254903 0.37254903 0.3882353  0.39215687 0.40392157 0.40784314\n",
            " 0.4117647  0.4509804  0.44313726 0.44313726 0.53333336 0.4745098\n",
            " 0.         0.2509804  0.58431375 0.49411765 0.35686275 0.41960785\n",
            " 0.4745098  0.42745098 0.47058824 0.46666667 0.49411765 0.5372549\n",
            " 0.4745098  0.6117647  0.6901961  0.5764706  0.5411765  0.50980395\n",
            " 0.4862745  0.4745098  0.4745098  0.4745098  0.45490196 0.46666667\n",
            " 0.47058824 0.46666667 0.5764706  0.5529412  0.         0.07058824\n",
            " 0.36862746 0.3882353  0.34901962 0.35686275 0.4745098  0.36078432\n",
            " 0.37254903 0.42352942 0.43529412 0.45882353 0.35686275 0.61960787\n",
            " 0.74509805 0.72156864 0.70980394 0.7019608  0.6784314  0.65882355\n",
            " 0.63529414 0.60784316 0.6039216  0.60784316 0.5529412  0.53333336\n",
            " 0.62352943 0.30980393 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ] [0 0 0 0 0 0 0 0 1 0]\n"
          ]
        }
      ],
      "source": [
        "# Now we try to load the images and their corresponding labels into memory\n",
        "\n",
        "def load_data(X, y):\n",
        "    for f in os.listdir(folder):\n",
        "        for file in os.listdir(f\"{folder}/{f}\"):\n",
        "            img = plt.imread(f\"{folder}/{f}/{file}\")\n",
        "            X.append(img)\n",
        "\n",
        "            # The most obvious choice for the label is the class (folder) name\n",
        "            # label = int(f)\n",
        "\n",
        "            # [Q1] But we dont use it here why? Why is it an array of 10 elements?\n",
        "            # Clue: Lookup one hot encoding\n",
        "            # Read up on Cross Entropy Loss\n",
        "\n",
        "            label = [0] * 10\n",
        "            label[int(f)] = 1 # Why is this array the label and not a numeber?\n",
        "\n",
        "            y.append(label)\n",
        "\n",
        "        print(f\"Loaded {f} class\")\n",
        "\n",
        "X, y = [], []\n",
        "load_data(X, y)\n",
        "\n",
        "# [Q2] Why convert to numpy array?\n",
        "\"\"\"\n",
        ".\n",
        "convert x and y to numpy arrays here\n",
        ".\n",
        "\"\"\"\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "X = X[:, :,:, 0] # [Q3] Why are we doing this and what does this type of slicing result in?\n",
        "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2]) # [Q4] Why are we reshaping the data?\n",
        "print(\"After reshaping\")\n",
        "print(X.shape, y.shape)\n",
        "print(X[0], y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j2IULseNdXg0"
      },
      "outputs": [],
      "source": [
        "class NN:\n",
        "    def __init__(self, input_neurons, hidden_neurons, output_neurons, learning_rate, epochs):\n",
        "        \"\"\"\n",
        "        Class Definition\n",
        "\n",
        "        We use a class because it is easy to visualize the process of training a neural network\n",
        "        It's also easier to resuse and repurpose depending on the task at hand\n",
        "\n",
        "        We have a simple neural network, with an input layer, one hidden (middle) layer and an output layer\n",
        "\n",
        "        input_neurons: Number of neurons in the input layer\n",
        "        hidden_neurons: Number of neurons in the hidden layer\n",
        "        output_neurons: Number of neurons in the output layer\n",
        "        learning_rate: The rate at which the weights are updated [Q5] What is the learning rate?\n",
        "        epochs: Number of times the model will train on the entire dataset\n",
        "        \"\"\"\n",
        "\n",
        "        self.input_neurons = input_neurons\n",
        "        self.hidden_neurons = hidden_neurons\n",
        "        self.output_neurons = output_neurons\n",
        "        self.epochs = epochs\n",
        "\n",
        "        self.lr = learning_rate\n",
        "\n",
        "        \"\"\"\n",
        "        Weights and Biases\n",
        "\n",
        "        At this point you should know what weights and biases are in a neural network and if not, go check out the 3blue1brown video on Neural Networks\n",
        "        What matters here is however the matrix dimensions of the weights and biases\n",
        "\n",
        "        [Q6] Why are the dimensions of the weights and biases the way they are?\n",
        "\n",
        "        Try to figure out the dimensions of the weights and biases for the hidden and output layers\n",
        "        Try to see what equations represent the forward pass (basically the prediction)\n",
        "        And then, try to see if the dimensions of the matrix multiplications are correct\n",
        "\n",
        "        Note: The bias dimensions may not match. Look up broadcasting in numpy to understand\n",
        "        [Q7] What is broadcasting and why do we need to broadcast the bias?\n",
        "        \"\"\"\n",
        "\n",
        "        # Ideally any random set of weights and biases can be used to initialize the network\n",
        "        # self.wih = np.random.randn(hidden_neurons, input_neurons)\n",
        "\n",
        "        # [Q8] What is np.random.randn? What's the shape of this matrix?\n",
        "\n",
        "        # Optional: Try to figure out why the weights are initialized this way\n",
        "        # Note: You can just use the commented out line above if you don't want to do this\n",
        "\n",
        "        self.wih = np.random.randn(hidden_neurons, input_neurons) * np.sqrt(2/input_neurons)\n",
        "        self.bih = np.zeros((hidden_neurons, 1))\n",
        "\n",
        "        self.who = np.random.randn(output_neurons, hidden_neurons) * np.sqrt(2/hidden_neurons)\n",
        "        self.bho = np.zeros((output_neurons, 1))\n",
        "\n",
        "    # Activation Functions and their derivatives\n",
        "    # [Q9] What are activation functions and why do we need them?\n",
        "\n",
        "    def relu(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the RELU function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return z * (z > 0)\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the Sigmoid function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def relu_derivative(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the RELU derivative function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return 1 * (z > 0)\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the Sigmoid derivative function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return z * (1 - z)\n",
        "\n",
        "    # [Q10] What is the softmax function and why do we need it? Read up on it\n",
        "    def softmax(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the Softmax function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
        "\n",
        "    def softmax_derivative(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the Softmax derivative function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "\n",
        "        z = z - np.max(z, axis=0, keepdims=True)  # stability trick\n",
        "        expz = np.exp(z)\n",
        "        return expz / np.sum(expz, axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "\n",
        "    # Loss Functions and their derivatives\n",
        "    # [Q11] What are loss functions and why do we need them?\n",
        "\n",
        "    def mean_squared_error(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        Implementation of the Mean Squared Error function\n",
        "        y: (10, n)\n",
        "        y_hat: (10, n)\n",
        "        returns (1, n)\n",
        "        \"\"\"\n",
        "        return np.mean((y - y_hat) ** 2, axis=0)\n",
        "\n",
        "    def cross_entropy_loss(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        Implementation of the Cross Entropy Loss function\n",
        "        y: (10, n)\n",
        "        y_hat: (10, n)\n",
        "        returns (1, n)\n",
        "        \"\"\"\n",
        "\n",
        "        # Implement the cross entropy loss function here and return it\n",
        "        # Keep the dimensions of the input in mind when writing the code\n",
        "\n",
        "        # [Code Goes Here]\n",
        "        return -np.sum(y * np.log(y_hat + 1e-9)) / y.shape[1]\n",
        "\n",
        "\n",
        "    def mean_squared_error_derivative(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        Implementation of the Mean Squared Error derivative function\n",
        "        y: (10, n)\n",
        "        y_hat: (10, n)\n",
        "        returns (10, n)\n",
        "        \"\"\"\n",
        "        return y_hat - y\n",
        "\n",
        "    def cross_entropy_derivative(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        Implementation of the Cross Entropy Loss derivative function\n",
        "        y: (10, n)\n",
        "        y_hat: (10, n)\n",
        "        returns (10, n)\n",
        "        \"\"\"\n",
        "\n",
        "        # Implement the cross entropy loss derivative function here and return it\n",
        "        # Note: The derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
        "\n",
        "        # [Code Goes Here]\n",
        "        return y_hat - y\n",
        "\n",
        "\n",
        "\n",
        "    # Forward propagation\n",
        "    def forward(self, input_list):\n",
        "        \"\"\"\n",
        "        Implementation of the Forward Pass\n",
        "        input_list: (784, n)        - n is the number of images\n",
        "        returns (10, n)              - n is the number of images\n",
        "\n",
        "        Now we come to the heart of the neural network, the forward pass\n",
        "        This is where the input is passed through the network to get the output\n",
        "\n",
        "        [Q12] What does the output choice we have here mean? It's an array of 10 elements per image, but why?\n",
        "        \"\"\"\n",
        "\n",
        "        inputs = np.array(input_list, ndmin=2).T\n",
        "        inputs = inputs - np.mean(inputs) # [Q13] Why are we subtracting the mean of the inputs?\n",
        "\n",
        "        # To get to the hidden layer:\n",
        "        # Multiply the input with the weights and adding the bias\n",
        "        # Apply the activation function (relu in this case)\n",
        "\n",
        "        # [Code Goes Here]\n",
        "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
        "        hidden_outputs = self.relu(hidden_inputs)\n",
        "\n",
        "\n",
        "        # To get to the output layer:\n",
        "        # Multiply the hidden layer output with the weights and adding the bias\n",
        "        # Apply the activation function (softmax in this case)\n",
        "        # [Q14] Why are we using the softmax function here?\n",
        "\n",
        "        # [Code Goes Here]\n",
        "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
        "        outputs = self.softmax(final_inputs)\n",
        "\n",
        "\n",
        "        # Return it\n",
        "\n",
        "        # [Code Goes Here]\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    # Back propagation\n",
        "    def backprop(self, inputs_list, targets_list):\n",
        "        \"\"\"\n",
        "        Implementation of the Backward Pass\n",
        "        inputs_list: (784, n)\n",
        "        targets_list: (10, n)\n",
        "        returns a scalar value (loss)\n",
        "\n",
        "        This is where the magic happens, the backpropagation algorithm\n",
        "        This is where the weights are updated based on the error in the prediction of the network\n",
        "\n",
        "        Now, the calculus involved is fairly complicated, especially because it's being done in matrix form\n",
        "        However the intuition is simple.\n",
        "\n",
        "        Since this is a recruitment stage, most of the function is written out for you, so follow along with the comments\n",
        "        \"\"\"\n",
        "\n",
        "        # Basic forward pass to get the outputs\n",
        "        # Obviously we need the predictions to know how the model is doing\n",
        "        # [Q15] Why are we doing a forward pass here instead of just using the outputs from the forward function?\n",
        "        # Is there any actual reason, or could we just swap it?\n",
        "\n",
        "        inputs = np.array(inputs_list, ndmin=2).T # (784, n)\n",
        "\n",
        "\n",
        "        tj = np.array(targets_list, ndmin=2).T # (10, n)\n",
        "\n",
        "\n",
        "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
        "        hidden_outputs = self.relu(hidden_inputs)\n",
        "\n",
        "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
        "        yj = self.softmax(final_inputs)\n",
        "\n",
        "        # Calculating the loss - This is the error in the prediction\n",
        "        # The loss then is the indication of how well the model is doing, its a useful parameter to track to see if the model is improving\n",
        "        output_errors = self.cross_entropy_derivative(tj, yj)\n",
        "        hidden_errors = np.dot(self.who.T, output_errors) * self.relu_derivative(hidden_inputs)\n",
        "\n",
        "        # loss = self.mean_squared_error(tj, yj) # Convert this to cross entropy loss\n",
        "        loss = self.cross_entropy_loss(tj, yj)\n",
        "\n",
        "\n",
        "        # Updating the weights using Update Rule\n",
        "        # Now that we have the incorrect predictions, we can update the weights to make the predictions better\n",
        "        # This is done using the gradient of the loss function with respect to the weights\n",
        "        # Basically, we know how much the overall error is caused due to individual weights using the chain rule of calculus\n",
        "        # Since we want to minimise the error, we move in the opposite direction of something like a \"derivative\" of the error with respect to the weights\n",
        "        # Calculus therefore helps us find the direction in which we should move to reduce the error\n",
        "        # A direction means what delta W changes we need to make to make the model better\n",
        "\n",
        "        # Output Layer - We start with the output layer because we are backtracking how the error is caused\n",
        "        # Think of it as using the derivatives of each layer while going back\n",
        "\n",
        "\n",
        "        # For the task, you will be using Cross Entropy Loss\n",
        "\n",
        "        # Change this to cross entropy loss\n",
        "        dE_dzo = yj - tj # (10,n)\n",
        "        # dE_dzo = self.mean_squared_error_derivative(tj, yj) * self.softmax_derivative(yj) # (10,n)\n",
        "        # Note: the derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
        "\n",
        "\n",
        "        dE_dwho = np.dot(dE_dzo, hidden_outputs.T) / hidden_outputs.shape[1] # dot((10,n) (n,128) = (10,128)\n",
        "        dE_dbho = np.mean(dE_dzo, axis=1, keepdims=True) # sum((10,n), axis=1) = (10,1)\n",
        "\n",
        "        self.who -= self.lr * dE_dwho\n",
        "        self.bho -= self.lr * dE_dbho\n",
        "\n",
        "        # Hidden Layer\n",
        "        dE_dah = np.dot(self.who.T, dE_dzo) # dot((128,10), (10,n)) = (128,n)\n",
        "        dE_dzh = dE_dah * self.relu_derivative(hidden_inputs)\n",
        "        dE_dwih = np.dot(dE_dzh, inputs.T) / inputs.shape[1]\n",
        "        dE_dbih = np.mean(dE_dzh, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "        self.wih -= self.lr * dE_dwih\n",
        "        self.bih -= self.lr * dE_dbih\n",
        "\n",
        "        return np.mean(loss)\n",
        "\n",
        "    def fit(self, inputs_list, targets_list,validation_data, validation_labels):\n",
        "        \"\"\"\n",
        "        Implementation of the training loop\n",
        "        inputs_list: (784, n)\n",
        "        targets_list: (10, n)\n",
        "        validation_data: (784, n)\n",
        "        validation_labels: (10, n)\n",
        "        returns train_loss, val_loss\n",
        "\n",
        "        This is where the training loop is implemented\n",
        "        We loop over the entire dataset for a certain number of epochs\n",
        "        We also track the validation loss to see how well the model is generalizing\n",
        "        [Q16] What is the validation dataset and what do we mean by generalization?\n",
        "\n",
        "        We also return the training and validation loss to see how the model is improving\n",
        "        It's a good idea to plot these to see how the model is doing\n",
        "        \"\"\"\n",
        "\n",
        "        train_loss = []\n",
        "        val_loss = []\n",
        "        for epoch in range(self.epochs):\n",
        "            loss = self.backprop(inputs_list, targets_list)\n",
        "            train_loss.append(loss)\n",
        "\n",
        "            #made a change here, calculated cross entropy loss of validation data, instead of mean squared loss\n",
        "            vloss = self.cross_entropy_loss(validation_labels.T, self.forward(validation_data))\n",
        "            val_loss.append(np.mean(vloss))\n",
        "            print(f\"Epoch: {epoch}, Loss: {loss}, Val Loss: {val_loss[-1]}\")\n",
        "\n",
        "        return train_loss[1:], val_loss[:-1]\n",
        "\n",
        "    def predict(self, X):\n",
        "        outputs = self.forward(X).T\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_8PszzgUdXg2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "9c199e98-12e8-4835-c5e2-fad4f466b6cd"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'NN' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1211460151.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# [Q17] What are the parameters in the model and what do they mean?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mfashion_mnist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'NN' is not defined"
          ]
        }
      ],
      "source": [
        "# This is where the class is used to train the model\n",
        "\n",
        "# The parameters in the model are (input_neurons, hidden_neurons, output_neurons, learning_rate, epochs)\n",
        "# These parameters aren't the right parameters, so tweak them to get the best results\n",
        "# Around 70% accuracy is a good end goal (75% is great) but for the recruitment task, 60% is good enough\n",
        "\n",
        "# [Q17] What are the parameters in the model and what do they mean?\n",
        "\n",
        "fashion_mnist = NN(784, 256, 10, 0.05, 50)\n",
        "p = np.random.permutation(len(X))\n",
        "X, y = X[p], y[p]\n",
        "\n",
        "# Splitting the data into training, validation and testing in the ratio 70:20:10\n",
        "X_train, y_train = X[:int(0.7*len(X))], y[:int(0.7*len(X))]\n",
        "X_val, y_val = X[int(0.7*len(X)):int(0.9*len(X))], y[int(0.7*len(X)):int(0.9*len(X))]\n",
        "X_test, y_test = X[int(0.9*len(X)):], y[int(0.9*len(X)):]\n",
        "\n",
        "# Training the model\n",
        "train_loss,val_loss = fashion_mnist.fit(X_train, y_train,X_val,y_val)\n",
        "\n",
        "\n",
        "# Plotting the loss\n",
        "plt.plot(train_loss,label='train')\n",
        "plt.plot(val_loss,label='val')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "y_pred = fashion_mnist.predict(X_test)\n",
        "\n",
        "# [Q18] Why are we using argmax here? Why is this output different from the output of the model?\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "print(f\"Accuracy: {np.mean(y_pred == y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A14kABVVdXg2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4977685-3526-4f96-f8be-bf84b97361dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6333333333333333\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy: {np.mean(y_pred == y_test)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}