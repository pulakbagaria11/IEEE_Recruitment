{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_UwZmhbwdXgy"
      },
      "outputs": [],
      "source": [
        "import os, numpy as np, matplotlib.pyplot as plt\n",
        "folder = \"./fashion-minst/final/\" # folder containing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9QgePQ3CdXg0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e234e64-03b4-461f-97d7-416e33906f8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 8 class\n",
            "Loaded 1 class\n",
            "Loaded 3 class\n",
            "Loaded 2 class\n",
            "Loaded 7 class\n",
            "Loaded 9 class\n",
            "Loaded 0 class\n",
            "Loaded 4 class\n",
            "Loaded 6 class\n",
            "Loaded 5 class\n",
            "After reshaping\n",
            "(60000, 784) (60000, 10)\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00784314\n",
            " 0.         0.         0.         0.34509805 0.42352942 0.4392157\n",
            " 0.43529412 0.24313726 0.         0.         0.00392157 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.02352941 0.         0.\n",
            " 0.7019608  0.5568628  0.         0.         0.02745098 0.39215687\n",
            " 0.4745098  0.         0.         0.00392157 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.00392157 0.         0.37254903 0.49019608 0.\n",
            " 0.         0.         0.         0.         0.4        0.31764707\n",
            " 0.         0.00784314 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.00392157 0.00392157 0.\n",
            " 0.         0.57254905 0.         0.         0.         0.\n",
            " 0.00392157 0.         0.         0.4745098  0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.00784314 0.00784314\n",
            " 0.00784314 0.00392157 0.00784314 0.         0.25490198 0.5529412\n",
            " 0.         0.01176471 0.         0.         0.01176471 0.00392157\n",
            " 0.         0.35686275 0.13725491 0.         0.00784314 0.\n",
            " 0.00784314 0.00392157 0.00392157 0.         0.00392157 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.34509805 0.2        0.         0.\n",
            " 0.         0.         0.         0.         0.         0.21960784\n",
            " 0.21960784 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00784314\n",
            " 0.09411765 0.07450981 0.09019608 0.09019608 0.08627451 0.04313726\n",
            " 0.45490196 0.20392157 0.01176471 0.06666667 0.08627451 0.07843138\n",
            " 0.07843138 0.07843138 0.         0.25490198 0.3254902  0.\n",
            " 0.05882353 0.04313726 0.02745098 0.02352941 0.00392157 0.\n",
            " 0.         0.00784314 0.         0.1882353  0.45882353 0.4392157\n",
            " 0.40784314 0.39215687 0.45882353 0.46666667 0.5137255  0.53333336\n",
            " 0.46666667 0.36078432 0.39215687 0.36078432 0.4        0.4\n",
            " 0.5019608  0.50980395 0.6392157  0.4745098  0.4745098  0.45490196\n",
            " 0.46666667 0.5764706  0.45882353 0.         0.         0.00392157\n",
            " 0.         0.17254902 0.47058824 0.3372549  0.27450982 0.24313726\n",
            " 0.37254903 0.37254903 0.1882353  0.30588236 0.40784314 0.34509805\n",
            " 0.44313726 0.4117647  0.44313726 0.27450982 0.45490196 0.23529412\n",
            " 0.2509804  0.41960785 0.34509805 0.28627452 0.3019608  0.4509804\n",
            " 0.41960785 0.         0.00392157 0.         0.         0.1882353\n",
            " 0.52156866 0.29411766 0.29411766 0.25490198 0.39215687 0.34901962\n",
            " 0.11764706 0.34509805 0.50980395 0.1882353  0.3019608  0.31764707\n",
            " 0.33333334 0.22745098 0.47058824 0.21960784 0.17254902 0.40784314\n",
            " 0.35686275 0.3254902  0.31764707 0.43529412 0.45882353 0.14117648\n",
            " 0.00392157 0.         0.         0.21176471 0.50980395 0.30980393\n",
            " 0.3019608  0.26666668 0.37254903 0.7607843  0.74509805 0.8392157\n",
            " 0.4745098  0.21960784 0.28627452 0.26666668 0.26666668 0.21960784\n",
            " 0.57254905 0.7921569  0.77254903 0.8117647  0.31764707 0.3372549\n",
            " 0.3372549  0.39215687 0.5686275  0.5568628  0.00392157 0.\n",
            " 0.         0.6117647  0.40784314 0.33333334 0.30980393 0.30980393\n",
            " 0.2784314  0.31764707 0.28627452 0.31764707 0.23529412 0.31764707\n",
            " 0.30980393 0.31764707 0.31764707 0.28627452 0.27450982 0.3882353\n",
            " 0.4        0.36862746 0.31764707 0.37254903 0.34117648 0.44313726\n",
            " 0.4862745  0.05098039 0.         0.         0.46666667 0.78431374\n",
            " 0.30980393 0.34509805 0.30980393 0.31764707 0.29411766 0.2784314\n",
            " 0.29411766 0.27450982 0.33333334 0.30588236 0.30980393 0.31764707\n",
            " 0.30980393 0.31764707 0.28627452 0.29411766 0.29411766 0.30588236\n",
            " 0.37254903 0.3764706  0.3764706  0.4392157  0.56078434 0.09411765\n",
            " 0.         0.         0.85490197 0.73333335 0.31764707 0.3372549\n",
            " 0.3019608  0.31764707 0.3372549  0.3254902  0.33333334 0.31764707\n",
            " 0.31764707 0.31764707 0.31764707 0.31764707 0.3254902  0.31764707\n",
            " 0.34509805 0.36862746 0.36078432 0.38039216 0.36862746 0.3764706\n",
            " 0.40392157 0.43529412 0.5764706  0.13725491 0.         0.3254902\n",
            " 0.85882354 0.4509804  0.43529412 0.3254902  0.30980393 0.34117648\n",
            " 0.33333334 0.31764707 0.3254902  0.3372549  0.34509805 0.34901962\n",
            " 0.34509805 0.34901962 0.34901962 0.35686275 0.38039216 0.3764706\n",
            " 0.38039216 0.3764706  0.4        0.4117647  0.4        0.46666667\n",
            " 0.5568628  0.25490198 0.         0.8862745  0.5254902  0.13725491\n",
            " 0.5568628  0.30588236 0.3254902  0.34117648 0.3372549  0.34509805\n",
            " 0.35686275 0.35686275 0.36078432 0.36862746 0.37254903 0.37254903\n",
            " 0.3764706  0.3764706  0.38039216 0.38039216 0.3882353  0.3882353\n",
            " 0.4        0.41960785 0.40392157 0.46666667 0.5411765  0.36862746\n",
            " 0.         1.         0.07450981 0.3764706  0.53333336 0.3254902\n",
            " 0.34901962 0.35686275 0.36078432 0.36862746 0.36862746 0.37254903\n",
            " 0.37254903 0.37254903 0.38039216 0.38039216 0.3882353  0.3882353\n",
            " 0.39215687 0.39215687 0.4        0.4        0.40392157 0.41960785\n",
            " 0.42352942 0.45490196 0.53333336 0.4392157  0.21176471 0.85490197\n",
            " 0.         0.50980395 0.5137255  0.34901962 0.3882353  0.37254903\n",
            " 0.3882353  0.3882353  0.3882353  0.3882353  0.39215687 0.3882353\n",
            " 0.3882353  0.39215687 0.39215687 0.4        0.4        0.40392157\n",
            " 0.40784314 0.40784314 0.40784314 0.42352942 0.4392157  0.4509804\n",
            " 0.5411765  0.4        0.30588236 0.63529414 0.         0.56078434\n",
            " 0.53333336 0.34901962 0.4        0.4        0.4117647  0.4117647\n",
            " 0.40784314 0.4117647  0.4117647  0.4        0.4        0.4\n",
            " 0.40784314 0.4        0.40784314 0.40784314 0.40784314 0.40392157\n",
            " 0.4        0.42745098 0.43529412 0.45882353 0.5568628  0.3372549\n",
            " 0.28627452 0.5411765  0.         0.5058824  0.5686275  0.34509805\n",
            " 0.40784314 0.42745098 0.42352942 0.4392157  0.42352942 0.43529412\n",
            " 0.4117647  0.40392157 0.40392157 0.40392157 0.40784314 0.39215687\n",
            " 0.40784314 0.4117647  0.41960785 0.41960785 0.4117647  0.4392157\n",
            " 0.43529412 0.45882353 0.5529412  0.35686275 0.05098039 0.4862745\n",
            " 0.         0.4392157  0.6117647  0.3372549  0.41960785 0.4392157\n",
            " 0.4117647  0.39215687 0.40392157 0.41960785 0.40392157 0.39215687\n",
            " 0.39215687 0.39215687 0.41960785 0.41960785 0.4        0.40392157\n",
            " 0.41960785 0.42352942 0.42352942 0.4509804  0.4392157  0.4392157\n",
            " 0.53333336 0.41960785 0.         0.34901962 0.7647059  0.5568628\n",
            " 0.67058825 0.5058824  0.6039216  0.63529414 0.65882355 0.62352943\n",
            " 0.62352943 0.60784316 0.5686275  0.46666667 0.38039216 0.37254903\n",
            " 0.37254903 0.37254903 0.3882353  0.39215687 0.40392157 0.40784314\n",
            " 0.4117647  0.4509804  0.44313726 0.44313726 0.53333336 0.4745098\n",
            " 0.         0.2509804  0.58431375 0.49411765 0.35686275 0.41960785\n",
            " 0.4745098  0.42745098 0.47058824 0.46666667 0.49411765 0.5372549\n",
            " 0.4745098  0.6117647  0.6901961  0.5764706  0.5411765  0.50980395\n",
            " 0.4862745  0.4745098  0.4745098  0.4745098  0.45490196 0.46666667\n",
            " 0.47058824 0.46666667 0.5764706  0.5529412  0.         0.07058824\n",
            " 0.36862746 0.3882353  0.34901962 0.35686275 0.4745098  0.36078432\n",
            " 0.37254903 0.42352942 0.43529412 0.45882353 0.35686275 0.61960787\n",
            " 0.74509805 0.72156864 0.70980394 0.7019608  0.6784314  0.65882355\n",
            " 0.63529414 0.60784316 0.6039216  0.60784316 0.5529412  0.53333336\n",
            " 0.62352943 0.30980393 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ] [0 0 0 0 0 0 0 0 1 0]\n"
          ]
        }
      ],
      "source": [
        "# Now we try to load the images and their corresponding labels into memory\n",
        "\n",
        "def load_data(X, y):\n",
        "    for f in os.listdir(folder):\n",
        "        for file in os.listdir(f\"{folder}/{f}\"):\n",
        "            img = plt.imread(f\"{folder}/{f}/{file}\")\n",
        "            X.append(img)\n",
        "\n",
        "            # The most obvious choice for the label is the class (folder) name\n",
        "            # label = int(f)\n",
        "\n",
        "            # [Q1] But we dont use it here why? Why is it an array of 10 elements?\n",
        "            # Clue: Lookup one hot encoding\n",
        "            # Read up on Cross Entropy Loss\n",
        "\n",
        "            label = [0] * 10\n",
        "            label[int(f)] = 1 # Why is this array the label and not a numeber?\n",
        "\n",
        "            y.append(label)\n",
        "\n",
        "        print(f\"Loaded {f} class\")\n",
        "\n",
        "X, y = [], []\n",
        "load_data(X, y)\n",
        "\n",
        "# [Q2] Why convert to numpy array?\n",
        "\"\"\"\n",
        ".\n",
        "convert x and y to numpy arrays here\n",
        ".\n",
        "\"\"\"\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "X = X[:, :,:, 0] # [Q3] Why are we doing this and what does this type of slicing result in?\n",
        "X = X.reshape(X.shape[0], X.shape[1]*X.shape[2]) # [Q4] Why are we reshaping the data?\n",
        "print(\"After reshaping\")\n",
        "print(X.shape, y.shape)\n",
        "print(X[0], y[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "j2IULseNdXg0"
      },
      "outputs": [],
      "source": [
        "class NN:\n",
        "    def __init__(self, input_neurons, hidden_neurons, output_neurons, learning_rate, epochs):\n",
        "        \"\"\"\n",
        "        Class Definition\n",
        "\n",
        "        We use a class because it is easy to visualize the process of training a neural network\n",
        "        It's also easier to resuse and repurpose depending on the task at hand\n",
        "\n",
        "        We have a simple neural network, with an input layer, one hidden (middle) layer and an output layer\n",
        "\n",
        "        input_neurons: Number of neurons in the input layer\n",
        "        hidden_neurons: Number of neurons in the hidden layer\n",
        "        output_neurons: Number of neurons in the output layer\n",
        "        learning_rate: The rate at which the weights are updated [Q5] What is the learning rate?\n",
        "        epochs: Number of times the model will train on the entire dataset\n",
        "        \"\"\"\n",
        "\n",
        "        self.input_neurons = input_neurons\n",
        "        self.hidden_neurons = hidden_neurons\n",
        "        self.output_neurons = output_neurons\n",
        "        self.epochs = epochs\n",
        "\n",
        "        self.lr = learning_rate\n",
        "\n",
        "        \"\"\"\n",
        "        Weights and Biases\n",
        "\n",
        "        At this point you should know what weights and biases are in a neural network and if not, go check out the 3blue1brown video on Neural Networks\n",
        "        What matters here is however the matrix dimensions of the weights and biases\n",
        "\n",
        "        [Q6] Why are the dimensions of the weights and biases the way they are?\n",
        "\n",
        "        Try to figure out the dimensions of the weights and biases for the hidden and output layers\n",
        "        Try to see what equations represent the forward pass (basically the prediction)\n",
        "        And then, try to see if the dimensions of the matrix multiplications are correct\n",
        "\n",
        "        Note: The bias dimensions may not match. Look up broadcasting in numpy to understand\n",
        "        [Q7] What is broadcasting and why do we need to broadcast the bias?\n",
        "        \"\"\"\n",
        "\n",
        "        # Ideally any random set of weights and biases can be used to initialize the network\n",
        "        # self.wih = np.random.randn(hidden_neurons, input_neurons)\n",
        "\n",
        "        # [Q8] What is np.random.randn? What's the shape of this matrix?\n",
        "\n",
        "        # Optional: Try to figure out why the weights are initialized this way\n",
        "        # Note: You can just use the commented out line above if you don't want to do this\n",
        "\n",
        "        self.wih = np.random.randn(hidden_neurons, input_neurons) * np.sqrt(2/input_neurons)\n",
        "        self.bih = np.zeros((hidden_neurons, 1))\n",
        "\n",
        "        self.who = np.random.randn(output_neurons, hidden_neurons) * np.sqrt(2/hidden_neurons)\n",
        "        self.bho = np.zeros((output_neurons, 1))\n",
        "\n",
        "    # Activation Functions and their derivatives\n",
        "    # [Q9] What are activation functions and why do we need them?\n",
        "\n",
        "    def relu(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the RELU function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return z * (z > 0)\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the Sigmoid function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def relu_derivative(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the RELU derivative function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return 1 * (z > 0)\n",
        "\n",
        "    def sigmoid_derivative(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the Sigmoid derivative function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return z * (1 - z)\n",
        "\n",
        "    # [Q10] What is the softmax function and why do we need it? Read up on it\n",
        "    def softmax(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the Softmax function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "        return np.exp(z) / np.sum(np.exp(z), axis=0)\n",
        "\n",
        "    def softmax_derivative(self, z):\n",
        "        \"\"\"\n",
        "        Implementation of the Softmax derivative function\n",
        "        z: (n, 1)\n",
        "        returns (n, 1)\n",
        "        \"\"\"\n",
        "\n",
        "        z = z - np.max(z, axis=0, keepdims=True)  # stability trick\n",
        "        expz = np.exp(z)\n",
        "        return expz / np.sum(expz, axis=0, keepdims=True)\n",
        "\n",
        "\n",
        "\n",
        "    # Loss Functions and their derivatives\n",
        "    # [Q11] What are loss functions and why do we need them?\n",
        "\n",
        "    def mean_squared_error(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        Implementation of the Mean Squared Error function\n",
        "        y: (10, n)\n",
        "        y_hat: (10, n)\n",
        "        returns (1, n)\n",
        "        \"\"\"\n",
        "        return np.mean((y - y_hat) ** 2, axis=0)\n",
        "\n",
        "    def cross_entropy_loss(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        Implementation of the Cross Entropy Loss function\n",
        "        y: (10, n)\n",
        "        y_hat: (10, n)\n",
        "        returns (1, n)\n",
        "        \"\"\"\n",
        "\n",
        "        # Implement the cross entropy loss function here and return it\n",
        "        # Keep the dimensions of the input in mind when writing the code\n",
        "\n",
        "        # [Code Goes Here]\n",
        "        return -np.sum(y * np.log(y_hat + 1e-9)) / y.shape[1]\n",
        "\n",
        "\n",
        "    def mean_squared_error_derivative(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        Implementation of the Mean Squared Error derivative function\n",
        "        y: (10, n)\n",
        "        y_hat: (10, n)\n",
        "        returns (10, n)\n",
        "        \"\"\"\n",
        "        return y_hat - y\n",
        "\n",
        "    def cross_entropy_derivative(self, y, y_hat):\n",
        "        \"\"\"\n",
        "        Implementation of the Cross Entropy Loss derivative function\n",
        "        y: (10, n)\n",
        "        y_hat: (10, n)\n",
        "        returns (10, n)\n",
        "        \"\"\"\n",
        "\n",
        "        # Implement the cross entropy loss derivative function here and return it\n",
        "        # Note: The derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
        "\n",
        "        # [Code Goes Here]\n",
        "        return y_hat - y\n",
        "\n",
        "\n",
        "\n",
        "    # Forward propagation\n",
        "    def forward(self, input_list):\n",
        "        \"\"\"\n",
        "        Implementation of the Forward Pass\n",
        "        input_list: (784, n)        - n is the number of images\n",
        "        returns (10, n)              - n is the number of images\n",
        "\n",
        "        Now we come to the heart of the neural network, the forward pass\n",
        "        This is where the input is passed through the network to get the output\n",
        "\n",
        "        [Q12] What does the output choice we have here mean? It's an array of 10 elements per image, but why?\n",
        "        \"\"\"\n",
        "\n",
        "        inputs = np.array(input_list, ndmin=2).T\n",
        "        inputs = inputs - np.mean(inputs) # [Q13] Why are we subtracting the mean of the inputs?\n",
        "\n",
        "        # To get to the hidden layer:\n",
        "        # Multiply the input with the weights and adding the bias\n",
        "        # Apply the activation function (relu in this case)\n",
        "\n",
        "        # [Code Goes Here]\n",
        "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
        "        hidden_outputs = self.relu(hidden_inputs)\n",
        "\n",
        "\n",
        "        # To get to the output layer:\n",
        "        # Multiply the hidden layer output with the weights and adding the bias\n",
        "        # Apply the activation function (softmax in this case)\n",
        "        # [Q14] Why are we using the softmax function here?\n",
        "\n",
        "        # [Code Goes Here]\n",
        "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
        "        outputs = self.softmax(final_inputs)\n",
        "\n",
        "\n",
        "        # Return it\n",
        "\n",
        "        # [Code Goes Here]\n",
        "        return outputs\n",
        "\n",
        "\n",
        "    # Back propagation\n",
        "    def backprop(self, inputs_list, targets_list):\n",
        "        \"\"\"\n",
        "        Implementation of the Backward Pass\n",
        "        inputs_list: (784, n)\n",
        "        targets_list: (10, n)\n",
        "        returns a scalar value (loss)\n",
        "\n",
        "        This is where the magic happens, the backpropagation algorithm\n",
        "        This is where the weights are updated based on the error in the prediction of the network\n",
        "\n",
        "        Now, the calculus involved is fairly complicated, especially because it's being done in matrix form\n",
        "        However the intuition is simple.\n",
        "\n",
        "        Since this is a recruitment stage, most of the function is written out for you, so follow along with the comments\n",
        "        \"\"\"\n",
        "\n",
        "        # Basic forward pass to get the outputs\n",
        "        # Obviously we need the predictions to know how the model is doing\n",
        "        # [Q15] Why are we doing a forward pass here instead of just using the outputs from the forward function?\n",
        "        # Is there any actual reason, or could we just swap it?\n",
        "\n",
        "        inputs = np.array(inputs_list, ndmin=2).T # (784, n)\n",
        "\n",
        "\n",
        "        tj = np.array(targets_list, ndmin=2).T # (10, n)\n",
        "\n",
        "\n",
        "        hidden_inputs = np.dot(self.wih, inputs) + self.bih\n",
        "        hidden_outputs = self.relu(hidden_inputs)\n",
        "\n",
        "        final_inputs = np.dot(self.who, hidden_outputs) + self.bho\n",
        "        yj = self.softmax(final_inputs)\n",
        "\n",
        "        # Calculating the loss - This is the error in the prediction\n",
        "        # The loss then is the indication of how well the model is doing, its a useful parameter to track to see if the model is improving\n",
        "        output_errors = self.cross_entropy_derivative(tj, yj)\n",
        "        hidden_errors = np.dot(self.who.T, output_errors) * self.relu_derivative(hidden_inputs)\n",
        "\n",
        "        # loss = self.mean_squared_error(tj, yj) # Convert this to cross entropy loss\n",
        "        loss = self.cross_entropy_loss(tj, yj)\n",
        "\n",
        "\n",
        "        # Updating the weights using Update Rule\n",
        "        # Now that we have the incorrect predictions, we can update the weights to make the predictions better\n",
        "        # This is done using the gradient of the loss function with respect to the weights\n",
        "        # Basically, we know how much the overall error is caused due to individual weights using the chain rule of calculus\n",
        "        # Since we want to minimise the error, we move in the opposite direction of something like a \"derivative\" of the error with respect to the weights\n",
        "        # Calculus therefore helps us find the direction in which we should move to reduce the error\n",
        "        # A direction means what delta W changes we need to make to make the model better\n",
        "\n",
        "        # Output Layer - We start with the output layer because we are backtracking how the error is caused\n",
        "        # Think of it as using the derivatives of each layer while going back\n",
        "\n",
        "\n",
        "        # For the task, you will be using Cross Entropy Loss\n",
        "\n",
        "        # Change this to cross entropy loss\n",
        "        dE_dzo = yj - tj # (10,n)\n",
        "        # dE_dzo = self.mean_squared_error_derivative(tj, yj) * self.softmax_derivative(yj) # (10,n)\n",
        "        # Note: the derivative of the CEL is usually taken with respect to the softmax input not output so keep that in mind while writing\n",
        "\n",
        "\n",
        "        dE_dwho = np.dot(dE_dzo, hidden_outputs.T) / hidden_outputs.shape[1] # dot((10,n) (n,128) = (10,128)\n",
        "        dE_dbho = np.mean(dE_dzo, axis=1, keepdims=True) # sum((10,n), axis=1) = (10,1)\n",
        "\n",
        "        self.who -= self.lr * dE_dwho\n",
        "        self.bho -= self.lr * dE_dbho\n",
        "\n",
        "        # Hidden Layer\n",
        "        dE_dah = np.dot(self.who.T, dE_dzo) # dot((128,10), (10,n)) = (128,n)\n",
        "        dE_dzh = dE_dah * self.relu_derivative(hidden_inputs)\n",
        "        dE_dwih = np.dot(dE_dzh, inputs.T) / inputs.shape[1]\n",
        "        dE_dbih = np.mean(dE_dzh, axis=1, keepdims=True)\n",
        "\n",
        "\n",
        "        self.wih -= self.lr * dE_dwih\n",
        "        self.bih -= self.lr * dE_dbih\n",
        "\n",
        "        return np.mean(loss)\n",
        "\n",
        "    def fit(self, inputs_list, targets_list,validation_data, validation_labels):\n",
        "        \"\"\"\n",
        "        Implementation of the training loop\n",
        "        inputs_list: (784, n)\n",
        "        targets_list: (10, n)\n",
        "        validation_data: (784, n)\n",
        "        validation_labels: (10, n)\n",
        "        returns train_loss, val_loss\n",
        "\n",
        "        This is where the training loop is implemented\n",
        "        We loop over the entire dataset for a certain number of epochs\n",
        "        We also track the validation loss to see how well the model is generalizing\n",
        "        [Q16] What is the validation dataset and what do we mean by generalization?\n",
        "\n",
        "        We also return the training and validation loss to see how the model is improving\n",
        "        It's a good idea to plot these to see how the model is doing\n",
        "        \"\"\"\n",
        "\n",
        "        train_loss = []\n",
        "        val_loss = []\n",
        "        for epoch in range(self.epochs):\n",
        "            loss = self.backprop(inputs_list, targets_list)\n",
        "            train_loss.append(loss)\n",
        "\n",
        "            #made a change here, calculated cross entropy loss of validation data, instead of mean squared loss\n",
        "            vloss = self.cross_entropy_loss(validation_labels.T, self.forward(validation_data))\n",
        "            val_loss.append(np.mean(vloss))\n",
        "            print(f\"Epoch: {epoch}, Loss: {loss}, Val Loss: {val_loss[-1]}\")\n",
        "\n",
        "        return train_loss[1:], val_loss[:-1]\n",
        "\n",
        "    def predict(self, X):\n",
        "        outputs = self.forward(X).T\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "_8PszzgUdXg2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "64508f56-e935-4ba2-fb61-a79498640f89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 2.466359548026429, Val Loss: 2.263675357589219\n",
            "Epoch: 1, Loss: 2.2202555727559177, Val Loss: 2.165399626227013\n",
            "Epoch: 2, Loss: 2.0879993036132727, Val Loss: 2.0826254010189027\n",
            "Epoch: 3, Loss: 1.9845670393129413, Val Loss: 2.0094830480879144\n",
            "Epoch: 4, Loss: 1.895225269201464, Val Loss: 1.943920706593776\n",
            "Epoch: 5, Loss: 1.815987082914682, Val Loss: 1.8847554018733357\n",
            "Epoch: 6, Loss: 1.7449555426041286, Val Loss: 1.831128102529793\n",
            "Epoch: 7, Loss: 1.680874690693756, Val Loss: 1.782334697172334\n",
            "Epoch: 8, Loss: 1.6228155609938058, Val Loss: 1.7377920814154877\n",
            "Epoch: 9, Loss: 1.570025097038876, Val Loss: 1.6969957185008788\n",
            "Epoch: 10, Loss: 1.5218524382784302, Val Loss: 1.6595319539850995\n",
            "Epoch: 11, Loss: 1.4778117695682678, Val Loss: 1.6250415167884584\n",
            "Epoch: 12, Loss: 1.4374695034031535, Val Loss: 1.5932132145407043\n",
            "Epoch: 13, Loss: 1.4004371051884257, Val Loss: 1.5637673660931257\n",
            "Epoch: 14, Loss: 1.3663596181740054, Val Loss: 1.5364713315654406\n",
            "Epoch: 15, Loss: 1.3349321971195593, Val Loss: 1.5111015217651762\n",
            "Epoch: 16, Loss: 1.3058694928034824, Val Loss: 1.4874681305326047\n",
            "Epoch: 17, Loss: 1.2789278403956912, Val Loss: 1.465410761325205\n",
            "Epoch: 18, Loss: 1.2538916496279382, Val Loss: 1.4447769998292892\n",
            "Epoch: 19, Loss: 1.2305684867474536, Val Loss: 1.425442018903054\n",
            "Epoch: 20, Loss: 1.2088028076150876, Val Loss: 1.4072856510149825\n",
            "Epoch: 21, Loss: 1.1884404984024537, Val Loss: 1.3902007075598461\n",
            "Epoch: 22, Loss: 1.169352314243092, Val Loss: 1.374100066807098\n",
            "Epoch: 23, Loss: 1.1514278643606128, Val Loss: 1.358904251788686\n",
            "Epoch: 24, Loss: 1.1345651202287006, Val Loss: 1.3445377921942665\n",
            "Epoch: 25, Loss: 1.1186691540437252, Val Loss: 1.3309376348992927\n",
            "Epoch: 26, Loss: 1.1036611504054896, Val Loss: 1.3180399683599886\n",
            "Epoch: 27, Loss: 1.08946707204828, Val Loss: 1.3057936941360722\n",
            "Epoch: 28, Loss: 1.0760184246819766, Val Loss: 1.2941506599095811\n",
            "Epoch: 29, Loss: 1.0632579595017837, Val Loss: 1.2830626260342044\n",
            "Epoch: 30, Loss: 1.051132284543083, Val Loss: 1.2724867321789752\n",
            "Epoch: 31, Loss: 1.0395899546927776, Val Loss: 1.2623915664855292\n",
            "Epoch: 32, Loss: 1.0285888160196979, Val Loss: 1.2527369162085502\n",
            "Epoch: 33, Loss: 1.0180874581671644, Val Loss: 1.2434973214389704\n",
            "Epoch: 34, Loss: 1.0080566068920453, Val Loss: 1.234645303943245\n",
            "Epoch: 35, Loss: 0.9984640073357627, Val Loss: 1.226154095339877\n",
            "Epoch: 36, Loss: 0.9892772739191151, Val Loss: 1.2180025419095923\n",
            "Epoch: 37, Loss: 0.9804701394098599, Val Loss: 1.2101705109136676\n",
            "Epoch: 38, Loss: 0.9720127158204592, Val Loss: 1.2026301451198818\n",
            "Epoch: 39, Loss: 0.9638877245892604, Val Loss: 1.195376043504391\n",
            "Epoch: 40, Loss: 0.9560778932858562, Val Loss: 1.1883886512858002\n",
            "Epoch: 41, Loss: 0.9485699753148552, Val Loss: 1.1816586574622152\n",
            "Epoch: 42, Loss: 0.9413449157433744, Val Loss: 1.1751622543618419\n",
            "Epoch: 43, Loss: 0.9343925324991011, Val Loss: 1.1688937305006157\n",
            "Epoch: 44, Loss: 0.9276945441063315, Val Loss: 1.1628406032085747\n",
            "Epoch: 45, Loss: 0.9212322108760603, Val Loss: 1.1569818088844064\n",
            "Epoch: 46, Loss: 0.9149895998382948, Val Loss: 1.1513140006151814\n",
            "Epoch: 47, Loss: 0.9089575635248707, Val Loss: 1.1458270619057864\n",
            "Epoch: 48, Loss: 0.9031264634167825, Val Loss: 1.1405089357981637\n",
            "Epoch: 49, Loss: 0.8974816581760588, Val Loss: 1.1353548998129421\n",
            "Epoch: 50, Loss: 0.8920166293602172, Val Loss: 1.1303531288170265\n",
            "Epoch: 51, Loss: 0.8867184412509953, Val Loss: 1.1254953197127373\n",
            "Epoch: 52, Loss: 0.8815782055424701, Val Loss: 1.1207796385136513\n",
            "Epoch: 53, Loss: 0.8765878352796499, Val Loss: 1.1161945131029947\n",
            "Epoch: 54, Loss: 0.8717387050823432, Val Loss: 1.1117323778098527\n",
            "Epoch: 55, Loss: 0.8670265056095665, Val Loss: 1.1073912647357655\n",
            "Epoch: 56, Loss: 0.8624440828430533, Val Loss: 1.103165710151213\n",
            "Epoch: 57, Loss: 0.8579854582870637, Val Loss: 1.0990520898255558\n",
            "Epoch: 58, Loss: 0.8536449844357772, Val Loss: 1.0950417365504397\n",
            "Epoch: 59, Loss: 0.8494168698880279, Val Loss: 1.0911306241974812\n",
            "Epoch: 60, Loss: 0.8452958568777796, Val Loss: 1.087311958942522\n",
            "Epoch: 61, Loss: 0.8412750686681788, Val Loss: 1.0835880583618693\n",
            "Epoch: 62, Loss: 0.8373504405178296, Val Loss: 1.0799508200516794\n",
            "Epoch: 63, Loss: 0.8335186977024265, Val Loss: 1.076398259140316\n",
            "Epoch: 64, Loss: 0.8297764812204365, Val Loss: 1.0729264799874287\n",
            "Epoch: 65, Loss: 0.8261210993486242, Val Loss: 1.0695320429992203\n",
            "Epoch: 66, Loss: 0.8225477754478195, Val Loss: 1.0662122683569344\n",
            "Epoch: 67, Loss: 0.8190539052282729, Val Loss: 1.062966395816545\n",
            "Epoch: 68, Loss: 0.815636000503094, Val Loss: 1.0597869910195754\n",
            "Epoch: 69, Loss: 0.8122923256189455, Val Loss: 1.0566777096002775\n",
            "Epoch: 70, Loss: 0.8090204360662077, Val Loss: 1.0536313758590548\n",
            "Epoch: 71, Loss: 0.8058168121404812, Val Loss: 1.0506477934405074\n",
            "Epoch: 72, Loss: 0.8026784328686739, Val Loss: 1.0477243315071905\n",
            "Epoch: 73, Loss: 0.7996027469258349, Val Loss: 1.0448592001461203\n",
            "Epoch: 74, Loss: 0.7965882419076549, Val Loss: 1.0420471776505138\n",
            "Epoch: 75, Loss: 0.7936327286140578, Val Loss: 1.0392906031063462\n",
            "Epoch: 76, Loss: 0.7907344043597699, Val Loss: 1.0365871202331431\n",
            "Epoch: 77, Loss: 0.7878914256697002, Val Loss: 1.0339340407932107\n",
            "Epoch: 78, Loss: 0.7851008582496849, Val Loss: 1.0313310850452442\n",
            "Epoch: 79, Loss: 0.7823624990845005, Val Loss: 1.0287777908419244\n",
            "Epoch: 80, Loss: 0.7796753163271787, Val Loss: 1.0262696677932264\n",
            "Epoch: 81, Loss: 0.7770376711545995, Val Loss: 1.023806023697746\n",
            "Epoch: 82, Loss: 0.7744466737742776, Val Loss: 1.021387496835819\n",
            "Epoch: 83, Loss: 0.7719006436552797, Val Loss: 1.019009368450006\n",
            "Epoch: 84, Loss: 0.7693980349560089, Val Loss: 1.0166777082387637\n",
            "Epoch: 85, Loss: 0.7669383611874522, Val Loss: 1.0143825239733661\n",
            "Epoch: 86, Loss: 0.7645195804490661, Val Loss: 1.0121268019059766\n",
            "Epoch: 87, Loss: 0.7621412775082385, Val Loss: 1.0099082764401532\n",
            "Epoch: 88, Loss: 0.7598016759847533, Val Loss: 1.0077259200429265\n",
            "Epoch: 89, Loss: 0.757499753205429, Val Loss: 1.0055801671809452\n",
            "Epoch: 90, Loss: 0.7552354610058809, Val Loss: 1.0034693728124917\n",
            "Epoch: 91, Loss: 0.7530076112837161, Val Loss: 1.0013907197019887\n",
            "Epoch: 92, Loss: 0.7508144830312881, Val Loss: 0.9993469268677242\n",
            "Epoch: 93, Loss: 0.7486555880648038, Val Loss: 0.9973332386843062\n",
            "Epoch: 94, Loss: 0.7465300101331203, Val Loss: 0.9953506904544757\n",
            "Epoch: 95, Loss: 0.7444367846790709, Val Loss: 0.9933996978314628\n",
            "Epoch: 96, Loss: 0.7423751671958034, Val Loss: 0.9914784605814146\n",
            "Epoch: 97, Loss: 0.7403443458531699, Val Loss: 0.9895873825780533\n",
            "Epoch: 98, Loss: 0.7383428060733384, Val Loss: 0.9877226941070439\n",
            "Epoch: 99, Loss: 0.7363704152263196, Val Loss: 0.9858875470338265\n",
            "Epoch: 100, Loss: 0.7344259548528671, Val Loss: 0.9840796263646727\n",
            "Epoch: 101, Loss: 0.732508978578932, Val Loss: 0.9822991354261487\n",
            "Epoch: 102, Loss: 0.7306184286854989, Val Loss: 0.980542277110817\n",
            "Epoch: 103, Loss: 0.7287540905725997, Val Loss: 0.9788137986540807\n",
            "Epoch: 104, Loss: 0.7269154588041202, Val Loss: 0.9771066794185346\n",
            "Epoch: 105, Loss: 0.7251015328752135, Val Loss: 0.9754253001035753\n",
            "Epoch: 106, Loss: 0.723312284196154, Val Loss: 0.9737702434486385\n",
            "Epoch: 107, Loss: 0.7215474353041096, Val Loss: 0.9721371614350416\n",
            "Epoch: 108, Loss: 0.7198062248435593, Val Loss: 0.9705253814929001\n",
            "Epoch: 109, Loss: 0.7180872887202363, Val Loss: 0.9689368286693616\n",
            "Epoch: 110, Loss: 0.7163906221133411, Val Loss: 0.9673691077226052\n",
            "Epoch: 111, Loss: 0.71471577908662, Val Loss: 0.9658235463021254\n",
            "Epoch: 112, Loss: 0.7130625006242594, Val Loss: 0.9642996311744185\n",
            "Epoch: 113, Loss: 0.7114295931491758, Val Loss: 0.9627950373478544\n",
            "Epoch: 114, Loss: 0.7098167395626225, Val Loss: 0.9613086055718411\n",
            "Epoch: 115, Loss: 0.7082239137628854, Val Loss: 0.9598437989843566\n",
            "Epoch: 116, Loss: 0.706650473936801, Val Loss: 0.9583970365513225\n",
            "Epoch: 117, Loss: 0.705095674308536, Val Loss: 0.9569706514837867\n",
            "Epoch: 118, Loss: 0.7035595299807872, Val Loss: 0.9555608821847543\n",
            "Epoch: 119, Loss: 0.7020412098934489, Val Loss: 0.9541696377598959\n",
            "Epoch: 120, Loss: 0.7005405319729339, Val Loss: 0.9527953149713476\n",
            "Epoch: 121, Loss: 0.6990575630183075, Val Loss: 0.9514379962109262\n",
            "Epoch: 122, Loss: 0.6975916416580971, Val Loss: 0.9500976965905304\n",
            "Epoch: 123, Loss: 0.6961422477674192, Val Loss: 0.9487762098035928\n",
            "Epoch: 124, Loss: 0.6947093298289692, Val Loss: 0.9474706711720067\n",
            "Epoch: 125, Loss: 0.6932927293471325, Val Loss: 0.9461816349131953\n",
            "Epoch: 126, Loss: 0.6918925742608684, Val Loss: 0.9449091702551681\n",
            "Epoch: 127, Loss: 0.6905082893790591, Val Loss: 0.9436512642461976\n",
            "Epoch: 128, Loss: 0.6891390562052706, Val Loss: 0.9424094964512333\n",
            "Epoch: 129, Loss: 0.687785156022098, Val Loss: 0.9411827781989864\n",
            "Epoch: 130, Loss: 0.6864460646389864, Val Loss: 0.9399691346194983\n",
            "Epoch: 131, Loss: 0.6851211865457404, Val Loss: 0.9387711898362945\n",
            "Epoch: 132, Loss: 0.6838106037968691, Val Loss: 0.9375888526895803\n",
            "Epoch: 133, Loss: 0.6825139040094818, Val Loss: 0.9364188366633602\n",
            "Epoch: 134, Loss: 0.6812307235897568, Val Loss: 0.9352630253041379\n",
            "Epoch: 135, Loss: 0.6799609432821893, Val Loss: 0.934122365261356\n",
            "Epoch: 136, Loss: 0.6787046680001937, Val Loss: 0.9329944395049872\n",
            "Epoch: 137, Loss: 0.6774610892817182, Val Loss: 0.9318799143053402\n",
            "Epoch: 138, Loss: 0.6762299471445702, Val Loss: 0.930778620552953\n",
            "Epoch: 139, Loss: 0.6750110839130483, Val Loss: 0.9296917111050179\n",
            "Epoch: 140, Loss: 0.6738044408171119, Val Loss: 0.9286173660977324\n",
            "Epoch: 141, Loss: 0.6726095886834657, Val Loss: 0.9275577391273299\n",
            "Epoch: 142, Loss: 0.6714262999970104, Val Loss: 0.9265093983119828\n",
            "Epoch: 143, Loss: 0.6702544848384392, Val Loss: 0.9254745111728524\n",
            "Epoch: 144, Loss: 0.6690944392994309, Val Loss: 0.9244498858042786\n",
            "Epoch: 145, Loss: 0.6679454947458461, Val Loss: 0.9234377676135405\n",
            "Epoch: 146, Loss: 0.6668077460039903, Val Loss: 0.9224359449526853\n",
            "Epoch: 147, Loss: 0.6656809049039597, Val Loss: 0.921444995430176\n",
            "Epoch: 148, Loss: 0.6645647893746918, Val Loss: 0.9204663318124497\n",
            "Epoch: 149, Loss: 0.6634588423658054, Val Loss: 0.9194984448806961\n",
            "Epoch: 150, Loss: 0.6623632184379586, Val Loss: 0.9185413107852646\n",
            "Epoch: 151, Loss: 0.6612776853364631, Val Loss: 0.9175943140272262\n",
            "Epoch: 152, Loss: 0.6602023185490125, Val Loss: 0.9166589099018926\n",
            "Epoch: 153, Loss: 0.6591365424628977, Val Loss: 0.915733323146314\n",
            "Epoch: 154, Loss: 0.6580804938415006, Val Loss: 0.9148182496850363\n",
            "Epoch: 155, Loss: 0.6570338970886019, Val Loss: 0.9139120002064574\n",
            "Epoch: 156, Loss: 0.655996768669985, Val Loss: 0.9130162283331043\n",
            "Epoch: 157, Loss: 0.6549688379249001, Val Loss: 0.9121288916877619\n",
            "Epoch: 158, Loss: 0.653949648370921, Val Loss: 0.911253429887682\n",
            "Epoch: 159, Loss: 0.6529393555733762, Val Loss: 0.9103860491126453\n",
            "Epoch: 160, Loss: 0.6519380579816524, Val Loss: 0.9095288545495864\n",
            "Epoch: 161, Loss: 0.650945294236412, Val Loss: 0.9086812577609518\n",
            "Epoch: 162, Loss: 0.6499611129861151, Val Loss: 0.9078447160757869\n",
            "Epoch: 163, Loss: 0.6489854800995645, Val Loss: 0.9070155332070328\n",
            "Epoch: 164, Loss: 0.6480177468344468, Val Loss: 0.9061959736432093\n",
            "Epoch: 165, Loss: 0.6470578121278749, Val Loss: 0.9053832248495566\n",
            "Epoch: 166, Loss: 0.6461056102941724, Val Loss: 0.9045814508496902\n",
            "Epoch: 167, Loss: 0.6451611103629556, Val Loss: 0.9037870733234904\n",
            "Epoch: 168, Loss: 0.6442244623388139, Val Loss: 0.9030020616261504\n",
            "Epoch: 169, Loss: 0.6432954779936555, Val Loss: 0.9022268691864763\n",
            "Epoch: 170, Loss: 0.6423738581254659, Val Loss: 0.9014581776874094\n",
            "Epoch: 171, Loss: 0.6414594977402034, Val Loss: 0.9006990182145197\n",
            "Epoch: 172, Loss: 0.6405524450423669, Val Loss: 0.8999474981746234\n",
            "Epoch: 173, Loss: 0.639652834173971, Val Loss: 0.8992049634769026\n",
            "Epoch: 174, Loss: 0.638760272651996, Val Loss: 0.8984696730130867\n",
            "Epoch: 175, Loss: 0.6378745188115134, Val Loss: 0.8977431135978953\n",
            "Epoch: 176, Loss: 0.6369957174709598, Val Loss: 0.8970248172452854\n",
            "Epoch: 177, Loss: 0.6361240484275883, Val Loss: 0.8963134753970508\n",
            "Epoch: 178, Loss: 0.6352592421743027, Val Loss: 0.8956106063722113\n",
            "Epoch: 179, Loss: 0.6344012877627652, Val Loss: 0.8949127557641959\n",
            "Epoch: 180, Loss: 0.6335498766064037, Val Loss: 0.89422348965221\n",
            "Epoch: 181, Loss: 0.632704820516326, Val Loss: 0.8935412905963086\n",
            "Epoch: 182, Loss: 0.6318661882774455, Val Loss: 0.8928664284750755\n",
            "Epoch: 183, Loss: 0.6310342904912681, Val Loss: 0.8921957362542774\n",
            "Epoch: 184, Loss: 0.6302087213787856, Val Loss: 0.8915348200270855\n",
            "Epoch: 185, Loss: 0.6293892580106402, Val Loss: 0.8908798281239365\n",
            "Epoch: 186, Loss: 0.628575690160698, Val Loss: 0.890232067941494\n",
            "Epoch: 187, Loss: 0.6277681245358971, Val Loss: 0.8895922693682944\n",
            "Epoch: 188, Loss: 0.6269666396993145, Val Loss: 0.8889585623682157\n",
            "Epoch: 189, Loss: 0.6261708730364189, Val Loss: 0.8883322297522763\n",
            "Epoch: 190, Loss: 0.6253807927533442, Val Loss: 0.8877122371504\n",
            "Epoch: 191, Loss: 0.624596360911559, Val Loss: 0.8870985872503199\n",
            "Epoch: 192, Loss: 0.6238176459140963, Val Loss: 0.8864902167891809\n",
            "Epoch: 193, Loss: 0.6230445187293132, Val Loss: 0.8858884534130274\n",
            "Epoch: 194, Loss: 0.6222772878234994, Val Loss: 0.8852938613449531\n",
            "Epoch: 195, Loss: 0.6215156580719688, Val Loss: 0.8847041460997916\n",
            "Epoch: 196, Loss: 0.620759328832598, Val Loss: 0.884120708384472\n",
            "Epoch: 197, Loss: 0.6200081805458987, Val Loss: 0.8835430849746085\n",
            "Epoch: 198, Loss: 0.619262200918676, Val Loss: 0.8829714418520436\n",
            "Epoch: 199, Loss: 0.6185213499548682, Val Loss: 0.8824064885266344\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7023333333333334\n"
          ]
        }
      ],
      "source": [
        "# This is where the class is used to train the model\n",
        "\n",
        "# The parameters in the model are (input_neurons, hidden_neurons, output_neurons, learning_rate, epochs)\n",
        "# These parameters aren't the right parameters, so tweak them to get the best results\n",
        "# Around 70% accuracy is a good end goal (75% is great) but for the recruitment task, 60% is good enough\n",
        "\n",
        "# [Q17] What are the parameters in the model and what do they mean?\n",
        "\n",
        "fashion_mnist = NN(784, 512, 10, 0.04, 200)\n",
        "p = np.random.permutation(len(X))\n",
        "X, y = X[p], y[p]\n",
        "\n",
        "# Splitting the data into training, validation and testing in the ratio 70:20:10\n",
        "X_train, y_train = X[:int(0.7*len(X))], y[:int(0.7*len(X))]\n",
        "X_val, y_val = X[int(0.7*len(X)):int(0.9*len(X))], y[int(0.7*len(X)):int(0.9*len(X))]\n",
        "X_test, y_test = X[int(0.9*len(X)):], y[int(0.9*len(X)):]\n",
        "\n",
        "# Training the model\n",
        "train_loss,val_loss = fashion_mnist.fit(X_train, y_train,X_val,y_val)\n",
        "\n",
        "\n",
        "# Plotting the loss\n",
        "plt.plot(train_loss,label='train')\n",
        "plt.plot(val_loss,label='val')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "y_pred = fashion_mnist.predict(X_test)\n",
        "\n",
        "# [Q18] Why are we using argmax here? Why is this output different from the output of the model?\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_test = np.argmax(y_test, axis=1)\n",
        "print(f\"Accuracy: {np.mean(y_pred == y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "A14kABVVdXg2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb772fb8-7f2c-4e65-e59d-3affb0e08441"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7023333333333334\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy: {np.mean(y_pred == y_test)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}